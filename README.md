<div align="center">
    <h1><b>A Survey on LLM Implicit Reasoning</b></h1>
</div>

The official GitHub page for the survey paper "A Survey on LLM Implicit Reasoning".


<div align="center">

![](https://img.shields.io/github/stars/jindongli-Ai/LLM-Implicit-Reasoning-Survey?color=yellow&cacheSeconds=60)
![](https://img.shields.io/github/forks/jindongli-Ai/LLM-Implicit-Reasoning-Survey?color=lightblue)
![](https://img.shields.io/github/last-commit/jindongli-Ai/LLM-Implicit-Reasoning-Survey?color=green)
![](https://img.shields.io/badge/PRs-Welcome-blue)
<a href="https://arxiv.org/" target="_blank"><img src="https://img.shields.io/badge/arXiv-xxxx.xxxxx-009688.svg" alt="arXiv"></a>

</div>
 

## Related Survey

### Reasoning

1. 2023_arXiv_Survey_A Survey of Reasoning with Foundation Model.
   [[arXiv]](https://arxiv.org/abs/2312.11562)
   [[Github]](https://github.com/reasoning-survey/Awesome-Reasoning-Foundation-Models)

2. 2024_EACL_Survey_Large Language Models for Mathematical Reasoning: Progresses and Challenges.
   [[ACL]](https://aclanthology.org/2024.eacl-srw.17/)
   [[arXiv]](https://arxiv.org/abs/2402.00157)

3. 2025_arXiv_Survey_Reinforced MLLM: A Survey on RL-Based Reasoning in Multimodal Large Language Models.
   [[arXiv]](https://arxiv.org/abs/2504.21277)

4. 2025_arXiv_Survey_Towards Reasoning Era: A Survey of Long-Chain-of-Thought for Reasoning Large Language Models.
   [[arXiv]](https://arxiv.org/abs/2503.09567)
   [[Github]](https://github.com/LightChen233/Awesome-Long-Chain-of-Thought-Reasoning)
   [[Homepage]](https://long-cot.github.io/)

5. 2025_arXiv_Survey_From System 1 to System 2: A Survey of Reasoning Large Language Models.
    [[arXiv]](https://arxiv.org/abs/2502.17419)
    [[Github]](https://github.com/zzli2022/Awesome-System2-Reasoning-LLM)

### Efficient Reasoning

1. 2025_arXiv_Survey_A Survey of Efficient Reasoning for Large Reasoning Models: Language, Multimodality, and Beyond.
   [[arXiv]](https://arxiv.org/abs/2503.21614)
   [[Github]](https://github.com/XiaoYee/Awesome_Efficient_LRM_Reasoning)
 
2. 2025_arXiv_Survey_Efficient Inference for Large Reasoning Models: A Survey.
   [[arXiv]](https://arxiv.org/abs/2503.23077)
   [[Github]](https://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs)
 
3. 2025_arXiv_Survey_Efficient Reasoning Models: A Survey.
   [[arXiv]](https://arxiv.org/abs/2504.10903)
   [[Github]](https://github.com/fscdc/Awesome-Efficient-Reasoning-Models)
 
4. 2025_arXiv_Survey_Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for Large Language Models.
   [[arXiv]](https://arxiv.org/abs/2503.24377)
   [[Github]](https://github.com/DevoAllen/Awesome-Reasoning-Economy-Papers)
 
5. 2025_arXiv_Survey_Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models.
    [[arXiv]](https://arxiv.org/abs/2503.16419)
    [[Github]](https://github.com/Eclipsess/Awesome-Efficient-Reasoning-LLMs)
 
6. 2025_arXiv_Survey_Reasoning Beyond Language: A Comprehensive Survey on Latent Chain-of-Thought Reasoning.
    [[arXiv]](https://arxiv.org/abs/2505.16782)
    [[Github]](https://github.com/EIT-NLP/Awesome-Latent-CoT)





## Related Reposority

### Reasoning

1. The-Martyr / Awesome-Multimodal-Reasoning.
   [[Github]](https://github.com/The-Martyr/Awesome-Multimodal-Reasoning)

2. atfortes / Awesome-LLM-Reasoning
   [[GitHub]](https://github.com/atfortes/Awesome-LLM-Reasoning)



### Efficient Reasoning

1. hemingkx / Awesome-Efficient-Reasoning.
   [[Github]](https://github.com/hemingkx/Awesome-Efficient-Reasoning)
   
2. Blueyee / Efficient-CoT-LRMs.
   [[Github]](https://github.com/Blueyee/Efficient-CoT-LRMs)

3. Hongcheng-Gao / Awesome-Long2short-on-LRMs.
   [[Github]](https://github.com/Hongcheng-Gao/Awesome-Long2short-on-LRMs)

4. zcccccz / Awesome-LLM-Implicit-Reasoning.
   [[Github]](https://github.com/zcccccz/Awesome-LLM-Implicit-Reasoning)

5. zzli2022 / Awesome-System2-Reasoning-LLM
   [[Github]](https://github.com/zzli2022/Awesome-System2-Reasoning-LLM)




## 3. Technical Paradigms for Implicit Reasoning

### 3.1 Latent-State Reasoning Representations

#### 3.1.1 Latent Representation Tokens as Reasoning Units

1. 2025_arXiv_CoCoMix_LLM Pretraining with Continuous Concepts.
   [[arXiv]](https://arxiv.org/abs/2502.08524v1)
   [[Github]](https://github.com/facebookresearch/RAM/tree/main/projects/cocomix)
   [[HuggingFace]](https://huggingface.co/papers/2502.08524)
   [[YouTube]](https://www.youtube.com/watch?v=3e-1mvDgQBI)
   [[Bilibili]](https://www.bilibili.com/video/BV1YWXGY8EPY/?vd_source=5a0ffee00ec6c37f96345e35b2838f32)
 
2. 2025_arXiv_Latent Token_Enhancing Latent Computation in Transformers with Latent Tokens.
   [[arXiv]](https://arxiv.org/abs/2505.12629)
   [[HuggingFace]](https://huggingface.co/papers/2505.12629)
   [[YouTube]](https://www.youtube.com/watch?v=h4TRfadNAFI)

3. 2025_ICML_LPC_Latent Preference Coding: Aligning Large Language Models via Discrete Latent Codes.
   [[ICML]](https://icml.cc/virtual/2025/poster/44849)
   [[arXiv]](https://arxiv.org/abs/2505.04993v1)
   
4. 2025_ICML_LTMs_Scalable Language Models with Posterior Inference of Latent Thought Vectors.
   [[ICML]](https://icml.cc/virtual/2025/poster/43587)
   [[arXiv]](https://arxiv.org/abs/2502.01567)
   [[Homepage]](https://deqiankong.github.io/blogs/ltm/)
   [[HuggingFace]](https://huggingface.co/papers/2502.01567)   
   
6. 2025_ICML_Token Assorted_Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning.
   [[ICML]](https://icml.cc/virtual/2025/poster/44409)
   [[arXiv]](https://arxiv.org/abs/2502.03275v1)
   [[HuggingFace]](https://huggingface.co/papers/2502.03275)



#### 3.1.2 Trajectory-Based Latent Reasoning

1. 2024_arXiv_CCoT_Compressed Chain of Thought: Efficient Reasoning through Dense Representations.
    [[arXiv]](https://arxiv.org/abs/2412.13171)
    [[HuggingFace]](https://huggingface.co/papers/2412.13171)

2. 2024_arXiv_HCoT_Expediting and Elevating Large Language Model Reasoning via Hidden Chain-of-Thought Decoding.
    [[arXiv]](https://arxiv.org/abs/2409.08561)
    [[HuggingFace]](https://huggingface.co/papers/2409.08561)

3. 2024_arXiv_Coconut_Training Large Language Models to Reason in a Continuous Latent Space.
    [[ICLR]](https://iclr.cc/virtual/2025/32772)
    [[arXiv]](https://arxiv.org/abs/2412.06769)
    [[Github]](https://github.com/facebookresearch/coconut)
    [[HuggingFace]](https://huggingface.co/papers/2412.06769)
    [[YouTube]](https://www.youtube.com/watch?v=mhKC3Avqy2E)

4. 2025_arXiv_CoT-Valve_CoT-Valve: Length-Compressible Chain-of-Thought Tuning.
    [[arXiv]](https://arxiv.org/abs/2502.09601)
    [[Code--Github]](https://github.com/horseee/CoT-Valve)

5. 2025_arXiv_CODI_CODI: Compressing Chain-of-Thought into Continuous Space via Self-Distillation.
    [[arXiv]](https://arxiv.org/abs/2502.21074)
    [[HuggingFace]](https://huggingface.co/papers/2502.21074)

6. 2025_arXiv_Heima_Efficient Reasoning with Hidden Thinking.
    [[arXiv]](https://arxiv.org/abs/2501.19201)
    [[Code--Github]](https://github.com/shawnricecake/Heima)
    [[HuggingFace]](https://huggingface.co/papers/2501.19201)
    [[YouTube]](https://www.youtube.com/watch?v=VYhjbzN_CGw)

7. 2025_arXiv_LightThinker_LightThinker: Thinking Step-by-Step Compression.
    [[arXiv]](https://arxiv.org/abs/2502.15589)
    [[Code--Github]](https://github.com/zjunlp/LightThinker)
    [[HuggingFace]](https://huggingface.co/papers/2502.15589)
    [[YouTube]](https://www.youtube.com/watch?v=NRVBkNMnG2k)

8. 2025_arXiv_PonderingLM_Pretraining Language Models to Ponder in Continuous Space.
    [[arXiv]](https://arxiv.org/abs/2505.20674)
    [[Github]](https://github.com/LUMIA-Group/PonderingLM)

9. 2025_arXiv_Soft Thinking_Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous Concept Space.
    [[arXiv]](https://arxiv.org/abs/2505.15778v1)
    [[HomePage]](https://soft-thinking.github.io/)
    [[Github]](https://github.com/eric-ai-lab/Soft-Thinking)
    [[HuggingFace]](https://huggingface.co/papers/2505.15778)
    [[YouTube]](https://www.youtube.com/watch?v=dgNut1AOadQ)
    [[Bilibili]](https://www.bilibili.com/video/BV1UvjzzSEKU/?vd_source=5a0ffee00ec6c37f96345e35b2838f32)

10. 2025_ACL_SoftCoT_SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs.
    [[arXiv]](https://arxiv.org/abs/2502.12134)
    [[Github]](https://github.com/xuyige/SoftCoT)
    [[HuggingFace]](https://huggingface.co/papers/2502.12134)
    [[Data-HuggingFace]](https://huggingface.co/datasets/xuyige/ASDiv-Aug)

11. 2025_arXiv_SoftCoT++_SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning.
    [[arXiv]](https://arxiv.org/abs/2505.11484)
    [[Github]](https://github.com/xuyige/SoftCoT)
    [[HuggingFace]](https://huggingface.co/papers/2505.11484)






#### 3.1.3 Non-Sequential Latent Reasoning (Diffusion / Memory)

1. 2024_NeruIPS_DoT_Diffusion of Thought: Chain-of-Thought Reasoning in Diffusion Language Models.
    [[NeurIPS]](https://proceedings.neurips.cc/paper_files/paper/2024/hash/be30024e7fa2c29cac7a6dafcbb8571f-Abstract-Conference.html)
    [[ACM NeurIPS]](https://dl.acm.org/doi/10.5555/3737916.3741259)
    [[arXiv]](https://proceedings.neurips.cc/paper_files/paper/2024/hash/be30024e7fa2c29cac7a6dafcbb8571f-Abstract-Conference.html)
    [[Github]](https://github.com/HKUNLP/diffusion-of-thoughts)
    [[HuggingFace]](https://huggingface.co/papers/2402.07754)
   
2. 2025_arXiv_CoT2_Continuous Chain of Thought Enables Parallel Exploration and Reasoning.
    [[arXiv]](https://arxiv.org/abs/2505.23648)
 
3. 2025_arXiv_DCoLT_Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models.
    [[arXiv]](https://arxiv.org/abs/2505.10446)
    [[HuggingFace]](https://huggingface.co/papers/2505.10446)
 
4. 2025_arXiv_Beyond Words_Beyond Words: A Latent Memory Approach to Internal Reasoning in LLMs.
    [[arXiv]](https://arxiv.org/abs/2502.21030)

5. 2025_arXiv_BoLT_Reasoning to Learn from Latent Thoughts.
    [[arXiv]](https://arxiv.org/abs/2503.18866)
    [[GitHub]](https://github.com/ryoungj/BoLT)
    [[HuggingFace]](https://huggingface.co/papers/2503.18866)
    [[YouTube]](https://www.youtube.com/watch?v=ON4VB6Wgqbw)
 
6. 2025_arXiv_ReaRec_Think Before Recommend: Unleashing the Latent Reasoning Power for Sequential Recommendation.
    [[arXiv]](http://export.arxiv.org/abs/2503.22675)
    [[HuggingFace]](https://huggingface.co/papers/2503.22675)
    [[YouTube]](https://www.youtube.com/watch?v=5i7qDcXeStU)





### 3.2 Control-Driven Implicit Reasoning

1. 2024_arXiv_Thinking Tokens for Language Modeling.
    [[arXiv]](https://arxiv.org/abs/2405.08644)
    [[HuggingFace]](https://huggingface.co/papers/2405.08644)
   
2. 2024_CoLM_FillerTokens_Let's Think Dot by Dot: Hidden Computation in Transformer Language Models.
    [[CoLM--OpenReview]](https://openreview.net/forum?id=NikbrdtYvG#discussion)
    [[arXiv]](https://arxiv.org/abs/2404.15758)
    [[GitHub]](https://github.com/jacobpfau/fillertokens)
    [[HuggingFace]](https://huggingface.co/papers/2404.15758)
    [[YouTube]](https://www.youtube.com/watch?v=AIR9QduDqD8)

4. 2024_CoLM_Guiding Language Model Reasoning with Planning Tokens.
    [[CoLM--OpenReview]](https://openreview.net/forum?id=wi9IffRhVM)
    [[arXiv]](https://arxiv.org/abs/2310.05707)
    [[Microsoft]](https://www.microsoft.com/en-us/research/publication/guiding-language-model-reasoning-with-planning-tokens/)
    [[Github]](https://github.com/WANGXinyiLinda/planning_tokens)
    

5. 2024_COLM_Quiet-STaR_Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking.
    [[CoLM--OpenReview]](https://openreview.net/forum?id=oRXPiSOGH9#discussion)
    [[arXiv]](https://arxiv.org/abs/2403.09629)
    [[Github]](https://github.com/ezelikman/quiet-star)
    [[HuggingFace]](https://huggingface.co/papers/2403.09629)
    [[YouTube]](https://www.youtube.com/watch?v=I78o3_lxXaQ)

6. 2024_ICLR_Think Before You Speak: Training Language Models with Pause Tokens.
    [[ICLR]](https://iclr.cc/virtual/2024/poster/17771)
    [[arXiv]](https://arxiv.org/abs/2310.02226)
    [[Github]](https://github.com/AkihikoWatanabe/paper_notes/issues/1072)
    [[HuggingFace]](https://huggingface.co/papers/2310.02226)
    [[YouTube]](https://www.youtube.com/watch?v=MtJ1jacr_yI)

7. 2025_arXiv_HRPO_Hybrid Latent Reasoning via Reinforcement Learning.
    [[arXiv]](https://arxiv.org/abs/2505.18454)
    [[Github]](https://github.com/skywalker-hub/HRPO1)
    [[HuggingFace]](https://huggingface.co/papers/2505.18454)
    
8. 2025_arXiv_LatentSeek_Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space.
    [[arXiv]](https://arxiv.org/abs/2505.13308v1)
    [[HomePage]](https://bigai-nlco.github.io/LatentSeek/)
    [[Github]](https://github.com/bigai-nlco/LatentSeek)
    [[HuggingFace]](https://huggingface.co/papers/2505.13308)
    
9. 2025_arXiv_System-1.5 Reasoning_System-1.5 Reasoning: Traversal in Language and Latent Spaces with Dynamic Shortcuts.
    [[arXiv]](https://arxiv.org/abs/2505.18962)
    [[HuggingFace]](https://huggingface.co/papers/2505.18962)




### 3.3 Recursive / Layer-Wise Reasoning Execution

   
1. 2025_arXiv_Huginn_Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach.
    [[arXiv]](https://arxiv.org/abs/2502.05171)
    [[HuggingFace]](https://huggingface.co/tomg-group-umd/huginn-0125)
    [[Github]](https://github.com/seal-rg/recurrent-pretraining)
    [[YouTube]](https://www.youtube.com/watch?v=-ZTXnQhH0PQ)

2. 2025_arXiv_ITT_Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster Adaptive Internal Thinking.
    [[arXiv]](https://arxiv.org/abs/2502.13842)
    [[HuggingFace]](https://huggingface.co/papers/2502.13842)
    [[YouTube]](https://www.youtube.com/watch?v=drbIAHOJiDc)
   
3. 2025_arXiv_RELAY_Enhancing Auto-regressive Chain-of-Thought through Loop-Aligned Reasoning.
    [[arXiv]](https://arxiv.org/abs/2502.08482)

4. 2025_ICLR_CoTFormer_CoTFormer: A Chain-of-Thought Driven Architecture with Budged-Adaptive Computation Cost at Inference.
    [[NeurIPS Workshop]](https://openreview.net/pdf?id=rBgo4Mi8vZ)
    [[ICLR]](https://iclr.cc/virtual/2025/poster/30808)
    [[arXiv]](https://arxiv.org/abs/2310.10845)
    [[Github]](https://github.com/epfml/cotformer)
   
5. 2025_ICLR_Reasoning with Latent Thoughts: On the Power of Looped Transformers.
    [[arXiv]](https://arxiv.org/abs/2502.17416)
    [[ICLR]](https://iclr.cc/virtual/2025/poster/28971)
    [[Poster]](https://iclr.cc/media/iclr-2025/Slides/28971.pdf)
    [[Youtube]](https://www.youtube.com/watch?v=S22Bs07HD0k)







### 3.4 Implicit Reasoning via Pretraining or Distillation

1. 2023_arXiv_ICoT-KD_Implicit Chain of Thought Reasoning via Knowledge Distillation.
    [[arXiv]](https://arxiv.org/abs/2311.01460)
    [[Microsoft]](https://www.microsoft.com/en-us/research/publication/implicit-chain-of-thought-reasoning-via-knowledge-distillation/)
    [[Github]](https://github.com/da03/implicit_chain_of_thought)
    [[HuggingFace]](https://huggingface.co/papers/2311.01460)
    [[YouTube]](https://www.youtube.com/watch?v=DDygkRmcxIk)

2. 2024_arXiv_ICoT-SI_From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step.
    [[arXiv]](https://arxiv.org/abs/2405.14838)
    [[Github]](https://github.com/sanowl/From-Explicit-CoT-to-Implicit-CoT-Learning-to-Internalize-CoT-Step-by-Step)
    [[HuggingFace]](https://huggingface.co/papers/2405.14838)
    [[YouTube]](https://www.youtube.com/live/llNI3mhg6BI)
    [[Bilibili]](https://www.bilibili.com/video/BV1WZ421M7sD/?vd_source=5a0ffee00ec6c37f96345e35b2838f32)

3. 2024_NeurIPS Workshop_Distilling System 2 into System 1.
    [[NeurIPS Workshop]](https://neurips.cc/virtual/2024/104303)
    [[arXiv]](https://arxiv.org/abs/2407.06023)
    [[YouTube]](https://www.youtube.com/watch?v=741gC9U9nW4)
   


<!--
    4. 2024_arXiv_Disentangling Memory and Reasoning Ability in Large Language Models.
        [[arXiv]](https://arxiv.org/abs/2411.13504)
        [[Github]](https://github.com/MingyuJ666/Disentangling-Memory-and-Reasoning)
    
    5. 2025_arXiv_DEBATER_Learning More Effective Representations for Dense Retrieval through Deliberate Thinking Before Search.
        [[arXiv]](https://arxiv.org/abs/2502.12974)
        [[Github]](https://github.com/OpenBMB/DEBATER)
       
    6. 2025_arXiv_Reasoning CPT_Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with Synthetic Data for LLM Reasoning.
        [[arXiv]](https://arxiv.org/abs/2505.10182)
-->






## 4. Mechanistic and Behavioral Evidence

### 4.1 Layer-wise Structural Evidence

1. 2024_LREC-COLING_arXiv_Jump to Conclusions: Short-Cutting Transformers with Linear Transformations.
    [[ACL-LREC-COLING]](https://aclanthology.org/2024.lrec-main.840/)
    [[arXiv]](https://arxiv.org/abs/2303.09435)
    [[Github]](https://github.com/sashayd/mat)
    [[HuggingFace]](https://huggingface.co/papers/2303.09435)
    
2. 2024_arXiv_Distributional Reasoning in LLMs: Parallel Reasoning Processes in Multi-Hop Reasoning.
    [[arXiv]](https://arxiv.org/abs/2406.13858)
    [[YouTube 1]](https://www.youtube.com/watch?v=tFcz-aTBqG0)
    [[]YouTbue 2](https://www.youtube.com/watch?v=SNw-Gh1LRFQ)
   
3. 2024_ACL_Do Large Language Models Latently Perform Multi-Hop Reasoning.
    [[ACL]](https://aclanthology.org/2024.acl-long.550/)
    [[arXiv]](https://arxiv.org/abs/2402.16837)
   
4. 2025_arXiv_Internal Chain-of-Thought: Empirical Evidence for Layer-wise Subtask Scheduling in LLMs.
    [[arXiv]](https://arxiv.org/abs/2505.14530)
    [[GitHub]](https://github.com/yzp11/internal-chain-of-thought)
   
6. 2025_arXiv_Implicit Reasoning in Transformers is Reasoning through Shortcuts.
    [[arXiv]](https://arxiv.org/abs/2503.07604)
    [[Github]](https://github.com/TianheL/LM-Implicit-Reasoning)
    [[HuggingFace]](https://huggingface.co/papers/2503.07604)
   
7. 2025_arXiv_Reasoning by Superposition: A Theoretical Perspective on Chain of Continuous Thought.
    [[arXiv]](https://arxiv.org/abs/2505.12514)
    
8. 2025_ICLR_To CoT or To Loop: A Formal Comparison Between Chain-of-Thought and Looped Transformers.
    [[ICLR--OpenReview]](https://openreview.net/forum?id=w6nlcS8Kkn)
    [[arXiv]](https://arxiv.org/abs/2505.19245)





### 4.2 Behavioral Signatures of Implicit Reasoning

1. 2024_arXiv_Do LLMs Really Think Step-by-Step in Implicit Reasoning.
    [[arXiv]](https://arxiv.org/abs/2411.15862)
    [[GitHub]](https://github.com/yuyijiong/if_step_by_step_implicit_cot)
   
3. 2024_NeurIPS_Can Language Models Learn to Skip Steps.
    [[NeurIPS]](https://proceedings.neurips.cc/paper_files/paper/2024/hash/504fa7e518da9d1b53a233ed20a38b46-Abstract-Conference.html)
    [[ACM NeurIPS]](https://dl.acm.org/doi/10.5555/3737916.3739357)
    [[arXiv]](https://arxiv.org/abs/2411.01855v1)
    [[Github]](https://github.com/tengxiaoliu/LM_skip)
    [[HuggingFace]](https://huggingface.co/papers/2411.01855)
   
4. 2024_arXiv_Think-to-Talk or Talk-to-Think: When LLMs Come Up with an Answer in Multi-Step Arithmetic Reasoning.
    [[arXiv]](https://arxiv.org/abs/2412.01113)
   
5. 2024_NeurIPS_Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization.
    [[ICML Workshop--OpenReview]](https://openreview.net/forum?id=ns8IH5Sn5y)
    [[NeurIPS]](https://proceedings.neurips.cc/paper_files/paper/2024/hash/ad217e0c7fecc71bdf48660ad6714b07-Abstract-Conference.html)
    [[arXiv]](https://arxiv.org/abs/2405.15071)
    [[Github]](https://github.com/OSU-NLP-Group/GrokkedTransformer)
    [[HuggingFace]](https://huggingface.co/papers/2405.15071)
    [[YouTube]](https://www.youtube.com/watch?v=qYcLhPnPezU)
   
6. 2025_Anthropic_On the Biology of Large Language Model.
    [[Anthropic]](https://transformer-circuits.pub/2025/attribution-graphs/biology.html)
    [[YouTube]](https://www.youtube.com/watch?v=mU3g2YPKlsA)
   





### 4.3 Representation-Based Probing of Reasoning

1. 2023_EMNLP_MechanisticProbe_Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models.
    [[EMNLP]](https://aclanthology.org/2023.emnlp-main.299/)
    [[arXiv]](https://arxiv.org/abs/2310.14491)
    [[Github]](https://github.com/yifan-h/mechanisticprobe)
    [[HuggingFace]](https://huggingface.co/papers/2310.14491)
   
2. 2024_ACL_A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task.
    [[ACL]](https://aclanthology.org/2024.findings-acl.242/)
    [[arXiv]](https://arxiv.org/abs/2402.11917v3)
    [[GitHub]](https://github.com/abhay-sheshadri/backward-chaining-circuits)
    [[HuggingFace]](https://huggingface.co/papers/2402.11917)
   
4. 2025_arXiv_Beyond Chains of Thought: Benchmarking Latent-Space Reasoning Abilities in Large Language Models.
    [[arXiv]](https://arxiv.org/abs/2504.10615)
    [[HuggingFace]](https://huggingface.co/papers/2504.10615)
   
5. 2025_ICLR Workshop_Uncovering Latent Chain of Thought Vectors in Large Language Models.
    [[ICLR Workshop]](https://iclr.cc/virtual/2025/33087)
    [[arXiv]](https://arxiv.org/abs/2409.14026)
   
7. 2025_ICLR_Latent Space Chain-of-Embedding Enables Output-free LLM Self-Evaluation.
    [[ICLR]](https://iclr.cc/virtual/2025/poster/28606)
    [[arXiv]](https://arxiv.org/abs/2410.13640)
    [[Github]](https://github.com/Alsace08/Chain-of-Embedding)






## Evaluation Methods and Benchmarks




## 📖 Citation















